## 第一天

#### 1.全连接神经网络

整体长什么样？单元长什么样？

#### 2.非线性激活函数

sigmoid、Tanh、relu、Leaky Relu

#### 3.前向传播

其实就是model的计算过程

#### 4.损失函数

公式

#### 5.梯度下降法

w、b参数更新公式

#### 6.反向传播计算

用链式求导推导公式

#### 7.卷积运算

图像在计算机中的本质是像素矩阵。

卷积运算过程：卷积核(w)、步幅、填充、输出特征图大小的计算公式

多通道的卷积运算

#### 8.池化操作

同样有类似卷积输出特征图的计算公式，没有参数！！

下采样：最大池化、平均池化。。。类似提取特征？压缩？

#### 9.卷积神经网络整体架构

输入层----卷积层----池化层----卷积层。。。

（=》这个过程叫feature extraction 特征提取）

\----全连接层----输出层。

## 第二天

#### 1.LeNet和AlexNet原理与实战

训练过程：搭建网络模型，输入到train.py和test.py进行训练测试，复用性比较强，后续随便替换模型都能跑

#### 2.LeNet的5层网络参数

输入层 ---- softmax(**卷积层** - 平均池化层) ---- softmax(**卷积层** - 平均池化层) ---- 激活函数(**全连接层) \*3**

注意每一层输出的特征大小计算

#### 3.LeNet总结

用**卷积做特征提取**，用全连接层做分类

## 第三天

神经网络可解释性差，很难推导为什么这种结构是好的，一般从训练数据的结果去解释这个模型好

#### 1.AlexNet结构

比lenet更多 更深

5层卷积 3全连接层，使用**relu**作为激活函数

#### 2.防止过拟合

dropout：某些训练轮次中是部分神经失活，利用修改后的模型输出反向传播修改原来的参数

#### 3.数据增强

翻转、随机裁剪、PCA

#### 4.LRN正则化

通道为方向。计算三个平方和

#### 5.LeNet实战

pytorch的nn库：卷积层、全连接神经层、激活函数。。

## 第四天

#### 1.高级卷积神经

GoogleNet：拼接

#### 2.RNN

##### 1.循环神经网络多用在时序数据分析，天气、语言文本、股市。。。

##### 2.结构图

```python
for x in X
	h = linear(x, h)
```

##### 3.RNN计算过程

##### 4.

图片怎么转换到文本：cnn卷积网络+全连接层 的输出直接传到RNN的输入端

## 第五天

LeNet训练代码流程

## 第六天

对之前训练代码的一些思考：

#### 1.为什么要mini batch训练数据、每个批次要独立更新梯度？

大批量一次性不现实，显存不支持、更新滞后、

**当于你在同时用多个 batch 叠加训练**，但因为没做平均，结果是不稳定的

*   **计算量太大**：需要把整个训练集的计算图都存起来，显存不够。
*   **更新频率太低**：只在 epoch 末尾更新一次权重，模型学得很慢（学习率等超参会崩）。
*   **收敛困难**：每轮梯度更新都滞后，模型可能震荡或陷入局部最优。

#### 2.那 batch size 不同，会不会影响损失或准确率？

**损失的绝对值会受 batch size 影响，**\
但我们通常会取平均（`mean`），而不是总和。

PyTorch 的默认 `reduction='mean'`，会自动除以 batch size，\
所以不管 batch 多大，**每个样本的平均损失都是可比的**。

*   **batch 越大，每步梯度的方差越小 收敛满 提高学习率（更稳定但更新慢）；**
*   **batch 越小，梯度波动更大 收敛快（更噪但可能跳出局部最优）。**

3.比如 batch=64，图像是 1×28×28，那：

    b_x.size() = torch.Size([64, 1, 28, 28])

所以 b\_x.size(0) == 64






# Transformer
![[Pasted image 20251109203843.png]]
## Input
1.收集文本，2.切割成token，3.统计频次排序， 4.由token生成独热向量

#### Embedding
词向量 = 词嵌入矩阵 x 独热向量

位置编码公式：
$$
\begin{aligned}
PE(pos, 2i) &= \sin\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right) \\
PE(pos, 2i+1) &= \cos\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right)
\end{aligned}
$$
![[Pasted image 20251109203822.png]]


## self-attention
1.计算输入词之间的关联程度
2.
![[Pasted image 20251109205850.png]]![[Pasted image 20251109210158.png]]